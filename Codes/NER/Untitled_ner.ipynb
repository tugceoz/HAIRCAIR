{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substring_range(s, substring):\n",
    "    for i in re.finditer(re.escape(substring), s):\n",
    "        yield (i.start(), i.end())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/tozturk/Desktop/HAIRCAIR/Codes/RedditScraper/TextData.csv')\n",
    "productList = pd.read_csv('/Users/tozturk/Desktop/HairProducts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I’m still basically on the beginners routine but I’ll include my little changes!\\n\\n1. Shampooed with dippity doo curl shampoo because I have fine hair and I find co-washing works for me about half the time and I need to shampoo the other half of the time\\n2. Applied rice water to my hair and wet plopped for ten-ish minutes and rinsed\\n3. Applied a handful of the tresseme botanical conditioner and gently finger combed my hair to get out knots. Sometimes I comb and repart my hair but I didn’t need to today\\n4. Flipped my head over and squished to condish, but then I have to flip it upright to comb a little at the crown of my head, because otherwise I get big patchy spots lol\\n5. Prayer hands applied herbal essences totally twisted gel and then scrunched\\n6. Wrapped my hair up in a microfiber towel turban so I can use body wash and just generally get all the hair product off my skin before getting out of the shower\\n7. Scrunched out as much water as possible with the towel turban\\n8. Diffused upside down for about ten minutes\\n9. Air dried the rest of the way and scrunched out the crunch!\\n\\nI'm only about 2 months into CG but the texture of my hair just looks so different than before! I laughed when I realized it was the same outfit and a similar hairstyle!\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1 = str(df.body[1])\n",
    "i2 = str(df.body[2])\n",
    "i3 = str(df.body[4])\n",
    "i4 = str(df.body[5])\n",
    "i4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(814, 826)]\n"
     ]
    }
   ],
   "source": [
    "print([x for x in substring_range(i3, \"rosemary oil\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'en'\n",
      "Losses {'ner': 6.025852323638901}\n",
      "Losses {'ner': 5.169803202639741}\n",
      "Losses {'ner': 3.376199821204864}\n",
      "Losses {'ner': 2.3826443790521807}\n",
      "Losses {'ner': 3.278167609200338}\n",
      "Losses {'ner': 2.531080874730833}\n",
      "Losses {'ner': 2.793932954916386}\n",
      "Losses {'ner': 1.8526069099598566}\n",
      "Losses {'ner': 2.543793841472052}\n",
      "Losses {'ner': 2.510759112245111}\n",
      "Losses {'ner': 2.7870500242705702}\n",
      "Losses {'ner': 2.65725796598781}\n",
      "Losses {'ner': 2.7056617249327246}\n",
      "Losses {'ner': 3.3047609198869736}\n",
      "Losses {'ner': 2.185876369791231}\n",
      "Losses {'ner': 1.738854438398107}\n",
      "Losses {'ner': 2.208912010179745}\n",
      "Losses {'ner': 3.001346165624909}\n",
      "Losses {'ner': 1.95690814628324}\n",
      "Losses {'ner': 1.6649432080922593}\n",
      "Losses {'ner': 1.1770130825596254}\n",
      "Losses {'ner': 2.309761112993444}\n",
      "Losses {'ner': 1.3872991871807336}\n",
      "Losses {'ner': 1.5121536562023548}\n",
      "Losses {'ner': 1.1455488057659144}\n",
      "Losses {'ner': 1.6723977007389266}\n",
      "Losses {'ner': 1.6369934211408506}\n",
      "Losses {'ner': 1.7616610759139049}\n",
      "Losses {'ner': 1.3766888597875706}\n",
      "Losses {'ner': 1.3123288779387354}\n",
      "Losses {'ner': 1.1813911857970005}\n",
      "Losses {'ner': 1.6788660389420018}\n",
      "Losses {'ner': 1.2271417352354703}\n",
      "Losses {'ner': 1.045395873861719}\n",
      "Losses {'ner': 1.441451217691126}\n",
      "Losses {'ner': 0.9473897342733506}\n",
      "Losses {'ner': 0.9795618306970932}\n",
      "Losses {'ner': 0.7660328339089975}\n",
      "Losses {'ner': 1.115702061672518}\n",
      "Losses {'ner': 0.6584354522998475}\n",
      "Losses {'ner': 1.1919349816264486}\n",
      "Losses {'ner': 1.6744806507922592}\n",
      "Losses {'ner': 0.6146481933111432}\n",
      "Losses {'ner': 0.5333748755603811}\n",
      "Losses {'ner': 0.46429058450351235}\n",
      "Losses {'ner': 1.4086962236756724}\n",
      "Losses {'ner': 1.4141192634976782}\n",
      "Losses {'ner': 1.089126195050968}\n",
      "Losses {'ner': 0.6466820349185355}\n",
      "Losses {'ner': 0.9769450989193089}\n",
      "Losses {'ner': 0.5542125261935382}\n",
      "Losses {'ner': 0.6552886280415084}\n",
      "Losses {'ner': 0.7004537805830837}\n",
      "Losses {'ner': 0.7612077793884744}\n",
      "Losses {'ner': 0.014910737063178888}\n",
      "Losses {'ner': 0.6538468629426583}\n",
      "Losses {'ner': 0.9673809508168743}\n",
      "Losses {'ner': 0.022779211085762377}\n",
      "Losses {'ner': 0.4125328111322327}\n",
      "Losses {'ner': 0.30833987501307075}\n",
      "Losses {'ner': 0.259336001420563}\n",
      "Losses {'ner': 0.1863459193650949}\n",
      "Losses {'ner': 0.6473515196483256}\n",
      "Losses {'ner': 0.3497878962780291}\n",
      "Losses {'ner': 0.42414138623529546}\n",
      "Losses {'ner': 0.16159768693925114}\n",
      "Losses {'ner': 0.35864985846197317}\n",
      "Losses {'ner': 0.057043482867947005}\n",
      "Losses {'ner': 0.016267933677596554}\n",
      "Losses {'ner': 0.7352675447441945}\n",
      "Losses {'ner': 0.22407484406314254}\n",
      "Losses {'ner': 0.22477079327592336}\n",
      "Losses {'ner': 0.22663189772578907}\n",
      "Losses {'ner': 0.18616095276536382}\n",
      "Losses {'ner': 0.3712719171234897}\n",
      "Losses {'ner': 0.451529747692886}\n",
      "Losses {'ner': 0.2591696857195046}\n",
      "Losses {'ner': 0.2359658140360897}\n",
      "Losses {'ner': 0.0036963341943927566}\n",
      "Losses {'ner': 0.257485846700652}\n",
      "Losses {'ner': 0.003150077808957339}\n",
      "Losses {'ner': 0.012738476444517418}\n",
      "Losses {'ner': 0.015819952056746654}\n",
      "Losses {'ner': 0.0008483336431872488}\n",
      "Losses {'ner': 0.0018880599151469132}\n",
      "Losses {'ner': 0.18077982728345313}\n",
      "Losses {'ner': 0.12726756762328445}\n",
      "Losses {'ner': 0.10238989836489754}\n",
      "Losses {'ner': 0.00014581665842171277}\n",
      "Losses {'ner': 0.01716387523774395}\n",
      "Losses {'ner': 0.21661932065435963}\n",
      "Losses {'ner': 0.00036060685662876735}\n",
      "Losses {'ner': 0.15793571024865777}\n",
      "Losses {'ner': 0.15690481920491073}\n",
      "Losses {'ner': 0.06125715519065257}\n",
      "Losses {'ner': 0.35662584155930277}\n",
      "Losses {'ner': 0.005771565168665423}\n",
      "Losses {'ner': 0.013155865505225694}\n",
      "Losses {'ner': 0.0001678993810772783}\n",
      "Losses {'ner': 0.17436063219589606}\n",
      "Entities [('Trader Joe’s conditioner-', 'PRODUCT'), ('TJ’s conditioner', 'PRODUCT'), ('citronella', 'PRODUCT'), ('rosemary oil', 'PRODUCT')]\n",
      "Tokens [('My', '', 2), ('routine', '', 2), (':', '', 2), ('\\n\\n', '', 2), ('-', '', 2), ('I', '', 2), ('only', '', 2), ('wash', '', 2), ('my', '', 2), ('hair', '', 2), ('about', '', 2), ('once', '', 2), ('a', '', 2), ('week', '', 2), (',', '', 2), ('and', '', 2), ('even', '', 2), ('then', '', 2), ('I', '', 2), ('try', '', 2), ('very', '', 2), ('hard', '', 2), ('to', '', 2), ('wash', '', 2), ('the', '', 2), ('scalp', '', 2), ('only', '', 2), ('.', '', 2), ('I', '', 2), ('use', '', 2), ('mild', '', 2), (',', '', 2), ('unscented', '', 2), ('shampoos', '', 2), ('.', '', 2), ('Honestly', '', 2), ('I', '', 2), ('do', '', 2), ('n’t', '', 2), ('pay', '', 2), ('attention', '', 2), ('to', '', 2), ('or', '', 2), ('have', '', 2), ('a', '', 2), ('preference', '', 2), ('for', '', 2), ('any', '', 2), ('name', '', 2), ('brands', '', 2), ('.', '', 2), ('I', '', 2), ('’ve', '', 2), ('tried', '', 2), ('them', '', 2), ('all', '', 2), (',', '', 2), ('cheap', '', 2), ('and', '', 2), ('expensive', '', 2), (',', '', 2), ('in', '', 2), ('my', '', 2), ('opinion', '', 2), (',', '', 2), ('the', '', 2), ('quality', '', 2), ('of', '', 2), ('conditioner', '', 2), ('makes', '', 2), ('way', '', 2), ('more', '', 2), ('of', '', 2), ('a', '', 2), ('difference', '', 2), ('.', '', 2), ('\\n\\n', '', 2), ('-Speaking', '', 2), ('of', '', 2), (',', '', 2), ('I', '', 2), ('use', '', 2), ('Trader', 'PRODUCT', 3), ('Joe', 'PRODUCT', 1), ('’s', 'PRODUCT', 1), ('conditioner-', 'PRODUCT', 1), ('about', '', 2), ('3', '', 2), ('dollars', '', 2), ('a', '', 2), ('bottle', '', 2), ('.', '', 2), ('I', '', 2), ('buy', '', 2), ('it', '', 2), ('by', '', 2), ('the', '', 2), ('case', '', 2), ('since', '', 2), ('I', '', 2), ('have', '', 2), ('two', '', 2), ('little', '', 2), ('ones', '', 2), ('with', '', 2), ('my', '', 2), ('same', '', 2), ('hair-', '', 2), ('we', '', 2), ('go', '', 2), ('through', '', 2), ('a', '', 2), ('lot', '', 2), ('of', '', 2), ('the', '', 2), ('stuff', '', 2), ('.', '', 2), ('I', '', 2), ('only', '', 2), ('comb', '', 2), ('my', '', 2), ('hair', '', 2), ('while', '', 2), ('it', '', 2), ('’s', '', 2), ('wet', '', 2), (',', '', 2), ('putting', '', 2), ('about', '', 2), ('twice', '', 2), ('as', '', 2), ('much', '', 2), ('conditioner', '', 2), ('as', '', 2), ('I', '', 2), ('need', '', 2), ('and', '', 2), ('rinsing', '', 2), ('out', '', 2), ('half', '', 2), ('.', '', 2), ('I', '', 2), ('wring', '', 2), ('it', '', 2), ('out', '', 2), ('and', '', 2), ('wrap', '', 2), ('it', '', 2), ('in', '', 2), ('a', '', 2), ('towel', '', 2), ('for', '', 2), ('about', '', 2), ('10', '', 2), ('minutes', '', 2), ('then', '', 2), ('air', '', 2), ('dry', '', 2), ('.', '', 2), ('That', '', 2), ('’s', '', 2), ('about', '', 2), ('it', '', 2), ('.', '', 2), ('\\n\\n', '', 2), ('-', '', 2), ('I', '', 2), ('keep', '', 2), ('a', '', 2), ('spray', '', 2), ('bottle', '', 2), ('of', '', 2), ('3', '', 2), ('parts', '', 2), ('water', '', 2), ('to', '', 2), ('one', '', 2), ('part', '', 2), ('TJ', 'PRODUCT', 3), ('’s', 'PRODUCT', 1), ('conditioner', 'PRODUCT', 1), ('.', '', 2), ('I', '', 2), ('add', '', 2), ('a', '', 2), ('few', '', 2), ('drops', '', 2), ('of', '', 2), ('citronella', 'PRODUCT', 3), ('and', '', 2), ('rosemary', 'PRODUCT', 3), ('oil', 'PRODUCT', 1), ('and', '', 2), ('use', '', 2), ('it', '', 2), ('as', '', 2), ('a', '', 2), ('detangling', '', 2), ('spray', '', 2), ('with', '', 2), ('my', '', 2), ('children-', '', 2), ('the', '', 2), ('oils', '', 2), ('supposedly', '', 2), ('work', '', 2), ('as', '', 2), ('a', '', 2), ('deterrent', '', 2), ('to', '', 2), ('head', '', 2), ('lice', '', 2), ('and', '', 2), ('also', '', 2), ('smell', '', 2), ('great', '', 2), ('.', '', 2), ('\\n\\n', '', 2), ('-', '', 2), ('I', '', 2), ('NEVER', '', 2), ('use', '', 2), ('heat', '', 2), ('or', '', 2), ('dye', '', 2), ('or', '', 2), ('any', '', 2), ('other', '', 2), ('products-', '', 2), ('not', '', 2), ('judging', '', 2), ('it', '', 2), ('’s', '', 2), ('just', '', 2), ('not', '', 2), ('my', '', 2), ('style', '', 2), ('.', '', 2), ('I', '', 2), ('do', '', 2), ('n’t', '', 2), ('know', '', 2), ('for', '', 2), ('sure', '', 2), ('if', '', 2), ('my', '', 2), ('hair', '', 2), ('is', '', 2), ('healthier', '', 2), ('because', '', 2), ('of', '', 2), ('this', '', 2), ('but', '', 2), ('I', '', 2), ('think', '', 2), ('so', '', 2), ('.', '', 2), ('Having', '', 2), ('Alopecia', '', 2), ('has', '', 2), ('made', '', 2), ('me', '', 2), ('pretty', '', 2), ('paranoid', '', 2), ('about', '', 2), ('damaging', '', 2), ('what', '', 2), ('little', '', 2), ('I', '', 2), ('have', '', 2), ('left', '', 2), ('so', '', 2), ('I', '', 2), ('’m', '', 2), ('very', '', 2), ('cautious', '', 2), ('.', '', 2), ('\\n\\n', '', 2), ('I', '', 2), ('hated', '', 2), ('my', '', 2), ('hair', '', 2), ('for', '', 2), ('most', '', 2), ('of', '', 2), ('my', '', 2), ('life', '', 2), ('.', '', 2), ('I', '', 2), ('grew', '', 2), ('up', '', 2), ('with', '', 2), ('my', '', 2), ('(', '', 2), ('white', '', 2), (')', '', 2), ('mom', '', 2), ('who', '', 2), ('had', '', 2), ('no', '', 2), ('idea', '', 2), ('how', '', 2), ('to', '', 2), ('handle', '', 2), ('it', '', 2), ('.', '', 2), ('Kids', '', 2), ('teased', '', 2), ('me', '', 2), ('and', '', 2), ('I', '', 2), ('did', '', 2), ('n’t', '', 2), ('see', '', 2), ('girls', '', 2), ('on', '', 2), ('magazines', '', 2), ('with', '', 2), ('hair', '', 2), ('like', '', 2), ('mine-', '', 2), ('aside', '', 2), ('from', '', 2), ('Ebony', '', 2), ('and', '', 2), ('Jet', '', 2), ('.', '', 2), ('\\n\\n', '', 2), ('Now', '', 2), ('natural', '', 2), ('hair', '', 2), ('is', '', 2), ('widely', '', 2), ('accepted', '', 2), ('and', '', 2), ('it', '', 2), ('makes', '', 2), ('me', '', 2), ('so', '', 2), ('happy', '', 2), ('for', '', 2), ('my', '', 2), ('children', '', 2), ('.', '', 2), ('Love', '', 2), ('this', '', 2), ('sub', '', 2), (',', '', 2), ('too', '', 2), (',', '', 2), ('y’', '', 2), ('all', '', 2), ('are', '', 2), ('gorgeous', '', 2), ('😍', '', 2), ('!', '', 2)]\n",
      "Entities [('Shea Moisture low-poo and conditioner', 'PRODUCT'), ('LA Looks extreme sport gel', 'PRODUCT')]\n",
      "Tokens [('Sticking', '', 2), ('to', '', 2), ('CG', '', 2), ('method', '', 2), ('exactly', '', 2), ('with', '', 2), ('Shea', 'PRODUCT', 3), ('Moisture', 'PRODUCT', 1), ('low', 'PRODUCT', 1), ('-', 'PRODUCT', 1), ('poo', 'PRODUCT', 1), ('and', 'PRODUCT', 1), ('conditioner', 'PRODUCT', 1), ('+', '', 2), ('LA', 'PRODUCT', 3), ('Looks', 'PRODUCT', 1), ('extreme', 'PRODUCT', 1), ('sport', 'PRODUCT', 1), ('gel', 'PRODUCT', 1), (',', '', 2), ('then', '', 2), ('plopping', '', 2), ('!', '', 2)]\n",
      "Entities [('Moisture Coconut & Hibiscus Curl Conditioner', 'PRODUCT'), ('Moisture Manuka Honey Deep Conditioner', 'PRODUCT'), (\"Not Your Mother's Kinky Moves Curl Defining Cream\", 'PRODUCT'), ('Deva Curl Volumizing mousse', 'PRODUCT'), ('Ultra Definining Cream', 'PRODUCT'), ('Cantu Comeback', 'PRODUCT')]\n",
      "Tokens [('*', '', 2), ('I', '', 2), (\"'ve\", '', 2), ('been', '', 2), ('playing', '', 2), ('with', '', 2), ('different', '', 2), ('products', '', 2), ('but', '', 2), ('this', '', 2), ('is', '', 2), ('what', '', 2), ('got', '', 2), ('me', '', 2), ('the', '', 2), ('after', '', 2), ('picture', '', 2), ('today', '', 2), ('!', '', 2), ('\\n\\n', '', 2), ('Wash', '', 2), ('day', '', 2), ('routine', '', 2), (':', '', 2), ('\\n', '', 2), ('-Not', '', 2), ('Your', '', 2), ('Mother', '', 2), (\"'s\", '', 2), ('Kinky', '', 2), ('Moves', '', 2), ('Shampoo', '', 2), ('\\n', '', 2), ('-She', '', 2), ('Moisture', 'PRODUCT', 3), ('Coconut', 'PRODUCT', 1), ('&', 'PRODUCT', 1), ('Hibiscus', 'PRODUCT', 1), ('Curl', 'PRODUCT', 1), ('Conditioner', 'PRODUCT', 1), ('\\n', '', 2), ('-Shea', '', 2), ('Moisture', 'PRODUCT', 3), ('Manuka', 'PRODUCT', 1), ('Honey', 'PRODUCT', 1), ('Deep', 'PRODUCT', 1), ('Conditioner', 'PRODUCT', 1), ('every', '', 2), ('other', '', 2), ('wash', '', 2), ('\\n', '', 2), ('-Squeeze', '', 2), ('out', '', 2), ('hair', '', 2), ('a', '', 2), ('little', '', 2), ('but', '', 2), ('and', '', 2), ('squish', '', 2), ('in', '', 2), ('Not', 'PRODUCT', 3), ('Your', 'PRODUCT', 1), ('Mother', 'PRODUCT', 1), (\"'s\", 'PRODUCT', 1), ('Kinky', 'PRODUCT', 1), ('Moves', 'PRODUCT', 1), ('Curl', 'PRODUCT', 1), ('Defining', 'PRODUCT', 1), ('Cream', 'PRODUCT', 1), ('\\n', '', 2), ('-Scrunch', '', 2), ('hair', '', 2), ('with', '', 2), ('microfiber', '', 2), ('towel', '', 2), ('and', '', 2), ('plop', '', 2), ('for', '', 2), ('about', '', 2), ('5', '', 2), ('-', '', 2), ('10', '', 2), ('minutes', '', 2), ('\\n', '', 2), ('-Rewet', '', 2), ('ends', '', 2), ('if', '', 2), ('they', '', 2), ('are', '', 2), ('looking', '', 2), ('dry', '', 2), ('before', '', 2), ('scrunching', '', 2), ('in', '', 2), ('Deva', 'PRODUCT', 3), ('Curl', 'PRODUCT', 1), ('Volumizing', 'PRODUCT', 1), ('mousse', 'PRODUCT', 1), ('and', '', 2), ('the', '', 2), ('Ultra', 'PRODUCT', 3), ('Definining', 'PRODUCT', 1), ('Cream', 'PRODUCT', 1), ('\\n', '', 2), ('-Finger', '', 2), ('coil', '', 2), ('and', '', 2), ('scrunch', '', 2), ('to', '', 2), ('get', '', 2), ('those', '', 2), ('ringlets', '', 2), ('!', '', 2), ('\\n', '', 2), ('-Put', '', 2), ('in', '', 2), ('root', '', 2), ('clips', '', 2), ('\\n', '', 2), ('Diffuse', '', 2), ('on', '', 2), ('low', '', 2), ('until', '', 2), ('almost', '', 2), ('dry', '', 2), ('.', '', 2), ('\\n\\n', '', 2), ('Refresh', '', 2), (':', '', 2), ('\\n', '', 2), ('-Spray', '', 2), ('everywhere', '', 2), ('until', '', 2), ('damp', '', 2), ('.', '', 2), ('\\n', '', 2), ('-Wet', '', 2), ('hands', '', 2), ('under', '', 2), ('faucet', '', 2), ('and', '', 2), ('spray', '', 2), ('a', '', 2), ('few', '', 2), ('pumps', '', 2), ('Cantu', 'PRODUCT', 3), ('Comeback', 'PRODUCT', 1), ('Next', '', 2), ('Day', '', 2), ('Curl', '', 2), ('Revitalizer', '', 2), ('into', '', 2), ('hands', '', 2), ('\\n', '', 2), ('-Run', '', 2), ('over', '', 2), ('hair', '', 2), ('and', '', 2), ('section', '', 2), ('off', '', 2), ('to', '', 2), ('finger', '', 2), ('curl', '', 2), ('areas', '', 2), ('that', '', 2), ('are', '', 2), ('getting', '', 2), ('poofy', '', 2), ('.', '', 2), ('Add', '', 2), ('more', '', 2), ('product', '', 2), ('for', '', 2), ('each', '', 2), ('section', '', 2), ('.', '', 2), ('\\n', '', 2), ('-Run', '', 2), ('a', '', 2), ('little', '', 2), ('more', '', 2), ('water', '', 2), ('and', '', 2), ('gel', '', 2), ('over', '', 2), ('hair', '', 2), ('to', '', 2), ('reduce', '', 2), ('frizz', '', 2), ('.', '', 2), ('\\n', '', 2), ('-Quick', '', 2), ('diffuse', '', 2), ('and', '', 2), ('you', '', 2), (\"'re\", '', 2), ('good', '', 2), ('to', '', 2), ('go', '', 2), ('!', '', 2), ('\\n\\n', '', 2)]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Could not determine the signature of None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-2d2c07060bf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mplac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/plac_core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(obj, arglist, eager, version)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0massociated\u001b[0m \u001b[0msubparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m     \"\"\"\n\u001b[0;32m--> 324\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         parser.add_argument(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/plac_core.py\u001b[0m in \u001b[0;36mparser_from\u001b[0;34m(obj, **confparams)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddsubcommands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'subcommands'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/plac_core.py\u001b[0m in \u001b[0;36mpopulate_from\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpopulated\u001b[0m \u001b[0mArgumentParser\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_func_argspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mdefaults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/plac_core.py\u001b[0m in \u001b[0;36m_set_func_argspec\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    238\u001b[0m         attribute to the parser. Also adds a .func reference to the object.\"\"\"\n\u001b[1;32m    239\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m         \u001b[0m_parser_registry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/plac_core.py\u001b[0m in \u001b[0;36mgetargspec\u001b[0;34m(callableobj)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         raise TypeError(_('Could not determine the signature of ') +\n\u001b[0;32m---> 38\u001b[0;31m                         str(callableobj))\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0margspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not determine the signature of None"
     ]
    }
   ],
   "source": [
    "i = df.body[1]\n",
    "\n",
    "# training data\n",
    "TRAIN_DATA = [\n",
    "    (i1, {\"entities\": [(35, 72, \"ORG\"), (75, 101, \"ORG\")]}),\n",
    "    (i2, {\"entities\": [(113, 142, \"ORG\"), (308, 337, \"ORG\"), (152, 200, \"ORG\"), (202, 245, \"ORG\"),\n",
    "                       (308,357, \"ORG\"), (483, 510, \"ORG\"), (519, 541, \"ORG\"), (729, 769, \"ORG\")]}),\n",
    "    (i3, {\"entities\": [(353, 377, \"ORG\"), (760, 776, \"ORG\"), (799, 809, \"ORG\"),\n",
    "                      (814, 826, \"ORG\")]}), \n",
    "]\n",
    "\n",
    "\n",
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n",
    ")\n",
    "def main(model=None, output_dir=None, n_iter=100):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        # reset and initialize the weights randomly – but only if we're\n",
    "        # training a new model\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in TRAIN_DATA:\n",
    "        doc = nlp(text)\n",
    "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        for text, _ in TRAIN_DATA:\n",
    "            doc = nlp2(text)\n",
    "            print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "            print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plac.call(main(model =  'en'))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"the little yellow dog barked at the cat\"\n",
    "grammar = ('''\n",
    "    NP: {<DT>?<JJ>*<NN>} # NP\n",
    "    ''')\n",
    "\n",
    "chunkParser = nltk.RegexpParser(grammar)\n",
    "tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "tagged"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
